[
  {
    "path": "posts/2022-08-06-css-tips-for-shiny/",
    "title": "CSS tips for Shiny",
    "description": "A collection of tips/ tricks for using CSS to tweak the UI of a shiny app. These are solutions\nI mostly look for when building shiny apps and want to improve the UI",
    "author": [
      {
        "name": "Lefkios  Paikousis",
        "url": "https://www.linkedin.com/in/lefkios/"
      }
    ],
    "date": "2022-08-06",
    "categories": [
      "CSS",
      "Shiny"
    ],
    "contents": "\r\n\r\nContents\r\nCenter a button\r\n(horizontaly)\r\n\r\n\r\nHere I will outline a collection of tips for using CSS to tweak the\r\nUI of a shiny app.\r\nI find myself looking on Google for the same things, over and over\r\nagain, so I decided to create a post to outline all the tips and tricks\r\nor resources when I want to do something with CSS on my Shiny app.\r\nI also hope that other people will benefit from this.\r\nThe code for all the examples can be found on the posts’\r\ngithub page\r\nCenter a button (horizontaly)\r\nOMG.. how many times I need to search for this\r\nactionButton(\"go\", \"Go\", style = \"display:block; margin:0 auto\")\r\nThis is the inline\r\nstyle.\r\nThe best is to use the File-based\r\nCSS\r\nLet’s say we add a class to the button called .center\r\n(or whatever class name you want)\r\nactionButton(\"go\", \"Go\", class = \".center')\r\nand on the CSS file, we add:\r\n.center{\r\n  \r\n  display:block; \r\n  margin: 0 auto;\r\n}\r\nTo find out how to center both horizontaly and vertically, visit this\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-08-06T17:27:10+03:00",
    "input_file": "css-tips-for-shiny.knit.md"
  },
  {
    "path": "posts/2022-07-27-how-fast-is-reading-parquet-files-with-arrow/",
    "title": "How fast is reading .parquet files with {arrow}",
    "description": "We compare reading times of csv files with different packages such as {readr},\n{data.table} and {arrow}. We also compare the reading time of a parquet data format with {arrow}.",
    "author": [
      {
        "name": "Lefkios  Paikousis",
        "url": "https://www.linkedin.com/in/lefkios/"
      }
    ],
    "date": "2022-07-27",
    "categories": [
      "arrow",
      ".parquet",
      ".csv",
      "data.table",
      "readr"
    ],
    "contents": "\r\nWhat I am primarily interested in, is the data format\r\n.parquet. It is advertised as an open source,\r\ncolumn-oriented data file format designed for efficient data storage and\r\nretrieval. Well, the column oriented seems to be aligned\r\nwith how R works.\r\nSee more info on the {arrow} package.\r\nWant to learn a bit more for Apache Arrow and dplyr for exploratory\r\ndata analysis, see this amazing\r\npresentation by Tom\r\nMock\r\nLast, the code for the entire analysis canbe found on github\r\nLet’s code now.\r\nFirst, the needed libraries.\r\n\r\n\r\nlibrary(arrow)\r\nlibrary(data.table)\r\nlibrary(readr)\r\n\r\nlibrary(palmerpenguins)\r\n\r\nlibrary(waldo) # to compare outputs when reading\r\nlibrary(gt)\r\nlibrary(glue)\r\nlibrary(tidyverse)\r\n\r\n\r\n\r\nThe dataset\r\nI am going to use the palmerpenguins dataset. It’s a dataset with 3\r\ncharacter columns and 5 numeric ones.\r\nWill do the following\r\nIncrease the row size to 100,000\r\nTurn factor columns to character\r\n\r\n\r\ndta <- palmerpenguins::penguins\r\n\r\nids <- sample(nrow(dta), 100000,  replace = TRUE)\r\n\r\ndta <- dta[ids,] %>% mutate(across(where(is.factor), as.character))\r\n\r\n\r\n\r\nWrite data to disk\r\nWe write a .csv version, and a .parquet\r\nversion\r\n\r\n\r\ntemp_file_csv <- tempfile(pattern = \".csv\")\r\n\r\ntemp_file_parquet <- tempfile(pattern = \".parquet\")\r\n\r\nwrite.csv(dta, temp_file_csv, row.names = FALSE)\r\n\r\narrow::write_parquet(dta, temp_file_parquet)\r\n\r\n\r\n\r\nThe comparison\r\nI use the {bench} package.\r\n\r\nNote the check = FALSE argument in\r\nbench::mark(). This argument checks whether the results of the different methods are consistent.\r\nThere is a reason I deactivate it, and I check manually later on\r\n\r\n\r\n\r\nset.seed(123)\r\n\r\nn_iterations <- 20\r\n\r\nres <- bench::mark(check = FALSE,\r\n                   \r\n                   \"base::read.csv\"        = read.csv(temp_file_csv),\r\n                   \"arrow::read_csv_arrow\" = arrow::read_csv_arrow(temp_file_csv),\r\n                   \"readr::read_csv\"       = readr::read_csv(\r\n                     temp_file_csv,  \r\n                     show_col_types = FALSE, \r\n                     progress = FALSE),\r\n                   \"data.table::fread\"     = data.table::fread(temp_file_csv),\r\n                   \"arrow::read_parquet\"   = arrow::read_parquet(temp_file_parquet),\r\n                   \r\n                   iterations = n_iterations\r\n)\r\n\r\n\r\n\r\nResults\r\nThe winner is: data.table::fread!\r\ndata.table::fread and arrow::read_parquet\r\nhave comparable speed.\r\nHowever, data.table consumes much more memory (5.87MB)\r\ncompared to arrow::read_parquet (1.08MB)\r\n\r\n\r\nres %>% \r\n  select(expression, min, median, mem_alloc)\r\n\r\n\r\n# A tibble: 5 x 4\r\n  expression                 min   median mem_alloc\r\n  <bch:expr>            <bch:tm> <bch:tm> <bch:byt>\r\n1 base::read.csv           184ms  187.7ms   26.88MB\r\n2 arrow::read_csv_arrow   30.4ms   32.2ms    5.36MB\r\n3 readr::read_csv        139.6ms  143.1ms    1.45MB\r\n4 data.table::fread       16.7ms   17.8ms    5.87MB\r\n5 arrow::read_parquet     15.8ms   16.7ms    1.08MB\r\n\r\n\r\n\r\n\r\n\r\nSize on disk\r\nSignificantly lower file size for the .parquet file\r\ncompared to the .csv.\r\n\r\n.csv size:  4.86 MB\r\n.parquet size:  0.43 MB\r\n\r\nDoes the content matter?\r\nA (rough) check whether the column type [characters or numbers] have\r\nany effect on the reading time.\r\nI create 3 datasets; one with mostly numeric columns, one with mostly\r\ncharacter columns and one with half numeric and half character\r\ncolumns\r\n\r\n\r\n# borrowed from \r\n# https://bench.r-lib.org/index.html#benchpress\r\ncreate_numeric_df <- function(rows, cols) {\r\n  as.data.frame(setNames(\r\n    replicate(cols, runif(rows, 1, 1000), simplify = FALSE),\r\n    rep_len(c(\"x\", letters), cols)))\r\n}\r\n\r\ncreate_character_df <- function(rows, cols) {\r\n  as.data.frame(setNames(\r\n    replicate(cols, sample(month.name, rows, replace = TRUE), simplify = FALSE),\r\n    rep_len(c(\"x\", LETTERS), cols)))\r\n}\r\n\r\n\r\nn_cols <- 10; n_rows <- 100000\r\n\r\nsome_string <- c(\"a string here\", \"a string there\")\r\n\r\n# Mostly numeric. 9 numeric and 1 character\r\ndta_numeric <- create_numeric_df(n_rows,n_cols) \r\ndta_numeric$x = some_string\r\n\r\n# Mostly character 9 character and 1 numeric\r\ndta_character <- create_character_df(n_rows, n_cols)\r\ndta_character$x <- runif(n_rows, 1, 1000)\r\n\r\n# Mixed. 5 numeric - 5 character\r\ndta_mixed <- bind_cols(\r\n  dta_numeric[letters[1:5]],\r\n  dta_character[LETTERS[1:5]]\r\n)\r\n\r\ntypes <- c(\"numeric\", \"character\", \"mixed\") %>% set_names()\r\n\r\ncsv_files <- imap(types, ~ tempfile(., fileext = \".csv\"))\r\n\r\nparquet_files <- imap(types, ~tempfile(., fileext = \".parquet\"))\r\n\r\n# write the scv files\r\nwalk2(csv_files, names(csv_files), function(path, type) {\r\n  \r\n  switch (type,\r\n          \"numeric\"   = write.csv(dta_numeric, path, row.names = FALSE),\r\n          \"character\" = write.csv(dta_character, path, row.names = FALSE),\r\n          \"mixed\"     = write.csv(dta_mixed, path, row.names = FALSE),\r\n          stop(\"No such type\")\r\n  )\r\n  \r\n})\r\n\r\n# write the .parquet files\r\nwalk2(parquet_files, names(parquet_files), function(path, type) {\r\n  \r\n  switch (type,\r\n          \"numeric\"   = arrow::write_parquet(dta_numeric, path),\r\n          \"character\" = arrow::write_parquet(dta_character, path),\r\n          \"mixed\"     = arrow::write_parquet(dta_mixed, path),\r\n          stop(\"No such type\")\r\n  )\r\n  \r\n})\r\n\r\n\r\n\r\nLet’s run the comparison\r\nI use the bench::press()\r\nto run a grid of comparisons across the 3 datasets.\r\n\r\n\r\nres_multi <- bench::press(\r\n  type = types,\r\n  {\r\n    set.seed(123)\r\n    \r\n    n_iterations <- 20\r\n    \r\n    res <- bench::mark(check = FALSE,\r\n                       \r\n                       \"base::read.csv\"          = read.csv(csv_files[[type]]),\r\n                       \"arrow::read_csv_arrow\"   = arrow::read_csv_arrow(csv_files[[type]]),\r\n                       \"readr::read_csv\"         = readr::read_csv(\r\n                         csv_files[[type]],  \r\n                         show_col_= FALSE, \r\n                         progress = FALSE),\r\n                       \"data.table::fread\"       = data.table::fread(csv_files[[type]]),\r\n                       \"arrow::read_parquet\"     = arrow::read_parquet(parquet_files[[type]]),\r\n                       \r\n                       iterations = n_iterations\r\n    )\r\n    \r\n  }\r\n)\r\n\r\n# don't forget to cleanup\r\nwalk(csv_files, unlink)\r\nwalk(parquet_files, unlink)\r\n\r\nggplot2::autoplot(res_multi, type = \"violin\")\r\n\r\n\r\n\r\n\r\nMedian reading times\r\n\r\n\r\nMedian reading times\r\n    100,000 rows dataset\r\n    expression\r\n      \r\n        Type of data\r\n      \r\n    numeric\r\n      character\r\n      mixed\r\n    arrow::read_parquet\r\n21.9ms\r\n34.0ms1\r\n26.3ms1data.table::fread\r\n22.1ms2\r\n43.5ms\r\n31.6msarrow::read_csv_arrow\r\n52.5ms\r\n56.1ms\r\n54.7msreadr::read_csv\r\n184.4ms\r\n178.9ms\r\n186.5msbase::read.csv\r\n767.4ms\r\n241.0ms\r\n505.6msbench::mark() results - code @ github\r\n    \r\n        \r\n          1\r\n          \r\n           \r\n          The fastest in mostly character or mixed data\r\n          \r\n        \r\n          2\r\n          \r\n           \r\n          The fastest in mostly numeric data\r\n          \r\n      \r\n    \r\n\r\nSame results, when\r\nreading the data files?\r\nLet’s check that we get the same data back, when reading from disk\r\nwith all these readers. I did use the\r\nbench::mark(check = TRUE) but it seems that when reading\r\nwith the readr::read_csv, I get the integer\r\ncolumns flipper_length_mm, body_mass_g and\r\nyear back as double\r\n\r\n\r\nreadr::read_csv(temp_file_csv) %>% map_chr(typeof)\r\n\r\n\r\n          species            island    bill_length_mm \r\n      \"character\"       \"character\"          \"double\" \r\n    bill_depth_mm flipper_length_mm       body_mass_g \r\n         \"double\"          \"double\"          \"double\" \r\n              sex              year \r\n      \"character\"          \"double\" \r\n\r\nwhereas the other readers (e.g. data.table::fread),\r\nreturn them as integer\r\n\r\n\r\ndata.table::fread(temp_file_csv)%>% map_chr(typeof)\r\n\r\n\r\n          species            island    bill_length_mm \r\n      \"character\"       \"character\"          \"double\" \r\n    bill_depth_mm flipper_length_mm       body_mass_g \r\n         \"double\"         \"integer\"         \"integer\" \r\n              sex              year \r\n      \"character\"         \"integer\" \r\n\r\nWhen I explicitly asked the readr::read_csv to read them\r\nas integer columns, then the only differences that remain\r\nare in the class of the object returned.\r\n\r\n\r\ncsv_base       = read.csv(temp_file_csv)\r\ncsv_arrow      = arrow::read_csv_arrow(temp_file_csv)\r\ncsv_readr      = readr::read_csv(temp_file_csv, \r\n                                 show_col_types = FALSE, progress = FALSE,\r\n                                 col_types = cols(\r\n                                   flipper_length_mm = col_integer(),\r\n                                   body_mass_g = col_integer(),\r\n                                   year = col_integer()\r\n                                 )\r\n)\r\ncsv_data_table = data.table::fread(temp_file_csv)\r\nparquet_arrow  = arrow::read_parquet(temp_file_parquet)\r\n\r\nreaders <- c(\"csv_base\",\"csv_readr\", \"csv_arrow\", \"csv_data_table\", \"parquet_arrow\")\r\n\r\ntbl_check <- expand.grid(\r\n  method1 = readers, method2 = readers, \r\n  stringsAsFactors = FALSE\r\n) %>% \r\n  filter(method1 <  method2) %>% \r\n  mutate(\r\n    comparison = map2(method1, method2, ~waldo::compare(\r\n                        get(.x), get(.y),\r\n                        x_arg = .x, y_arg = .y\r\n                      )))\r\n\r\ntbl_check$comparison\r\n\r\n\r\n[[1]]\r\n`class(csv_arrow)`: \"tbl_df\" \"tbl\" \"data.frame\"\r\n`class(csv_base)`:                 \"data.frame\"\r\n\r\n[[2]]\r\n`class(csv_base)`:                               \"data.frame\"\r\n`class(csv_readr)`: \"spec_tbl_df\" \"tbl_df\" \"tbl\" \"data.frame\"\r\n\r\n[[3]]\r\n`class(csv_arrow)`:               \"tbl_df\" \"tbl\" \"data.frame\"\r\n`class(csv_readr)`: \"spec_tbl_df\" \"tbl_df\" \"tbl\" \"data.frame\"\r\n\r\n[[4]]\r\n`class(csv_data_table)`: \"data.table\"                 \"data.frame\"\r\n`class(csv_readr)`:      \"spec_tbl_df\" \"tbl_df\" \"tbl\" \"data.frame\"\r\n\r\n[[5]]\r\n`class(csv_base)`:                    \"data.frame\"\r\n`class(csv_data_table)`: \"data.table\" \"data.frame\"\r\n\r\n[[6]]\r\n`class(csv_arrow)`:      \"tbl_df\"     \"tbl\" \"data.frame\"\r\n`class(csv_data_table)`: \"data.table\"       \"data.frame\"\r\n\r\n[[7]]\r\n`class(csv_base)`:                     \"data.frame\"\r\n`class(parquet_arrow)`: \"tbl_df\" \"tbl\" \"data.frame\"\r\n\r\n[[8]]\r\n`class(csv_readr)`:     \"spec_tbl_df\" \"tbl_df\" \"tbl\" \"data.frame\"\r\n`class(parquet_arrow)`:               \"tbl_df\" \"tbl\" \"data.frame\"\r\n\r\n[[9]]\r\nv No differences\r\n\r\n[[10]]\r\n`class(csv_data_table)`: \"data.table\"       \"data.frame\"\r\n`class(parquet_arrow)`:  \"tbl_df\"     \"tbl\" \"data.frame\"\r\n\r\nDon’t forget to clean up people\r\n\r\n\r\nunlink(temp_file_csv)\r\nunlink(temp_file_parquet)\r\n\r\n\r\n\r\nThat’s it! Hope you enjoyed it\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-27-how-fast-is-reading-parquet-files-with-arrow/how-fast-is-arrow_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2022-07-31T13:22:32+03:00",
    "input_file": "how-fast-is-arrow.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-04-distill-resources/",
    "title": "Distill resources",
    "description": "Resources to improve the distill website",
    "author": [
      {
        "name": "Lefkios  Paikousis",
        "url": "https://www.linkedin.com/in/lefkios/"
      }
    ],
    "date": "2021-01-04",
    "categories": [],
    "contents": "\r\nSome resouces to use to improve my distill website\r\nDistill by Rstudio The official documentation and guide on how to create the blog\r\nDistill css defaults\r\nThis blog\r\nCheck out the code for this blog, also the _site.yml file(https://github.com/shamindras/ss_personal_distill_blog/blob/master/_site.yml).\r\nThanks Shamindra Shrotriya\r\nThomas Mock blog\r\nLot’s can be learned from this blog and its code.\r\nHis blog is here\r\nArticle Metadata\r\nFigures, twitter, google scloar..\r\nOther Metadata\r\nCitatios, footnote, tabke of contents, appendices, etc..\r\nImages\r\nPreview Photo by Austin Distel on Unsplash\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-04-distill-resources/austin-distel-4r72LPFh4Ik-unsplash_small.jpg",
    "last_modified": "2021-01-04T14:12:26+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-23-a-workflow-with-labelled-data/",
    "title": "A workflow with labelled data",
    "description": "I love labelled data.",
    "author": [
      {
        "name": "Lefkios  Paikousis",
        "url": "https://www.linkedin.com/in/lefkios/"
      }
    ],
    "date": "2020-12-23",
    "categories": [
      "labelled-data",
      "workflow",
      "labels",
      "ggplot2"
    ],
    "contents": "\r\n\r\nContents\r\nThe Libraries\r\nRead the data and data-dictionary\r\nFactors and order of the value labels\r\nPlots\r\nAxis titles\r\nFacet strip labels\r\nAxis text labels\r\n\r\n\r\nThe Libraries\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(labelled)\r\nlibrary(gtsummary)\r\nlibrary(readxl)\r\n\r\n\r\n\r\nI usually work with survey data that come in the form of a spreadsheet (usually from google forms).\r\nThe variable names are long and highly descriptive of what information the variable contains, but that long of a name, makes it very hard to work with while wrangling the dataset.\r\nWhat I want is to keep the original variable name as a label and rename the variable names with new names\r\nTo demontrate my workflow, I created a sample dataset to work with. You can download it from github\r\nThe dataset is stored as an excel file, and has two worksheets.\r\nIn the worksheet data you can see the raw data, and\r\nIn the worksheet labels you can find a sort of data dictionary\r\nIn the labels worksheet I keep a record of the original variable names as label and on the adjacent column, I type a short but readable column name for the variable.\r\nworksheet:labelsRead the data and data-dictionary\r\nLet’s read in the data\r\n\r\n\r\npath = here::here(\"Data\", \"sample_survey.xlsx\")\r\ndta <- readxl::read_xlsx(path)\r\n\r\nglimpse(dta)\r\n\r\n\r\nRows: 7\r\nColumns: 6\r\n$ id                                   <dbl> 1, 2, 3, 4, 5, 6, 7\r\n$ `Which restaurant have you visited?` <chr> \"Limassol\", \"Nicosia\", ~\r\n$ `I am happy with the greeting`       <chr> \"Agree\", \"Strongly disa~\r\n$ `I enjoyed the food`                 <chr> \"Neutral\", \"Neutral\", \"~\r\n$ `I am  satisfied with the cost`      <chr> \"Agree\", \"Strongly disa~\r\n$ `How satisfied you are overall`      <chr> \"Very satisfied\", \"Very~\r\n\r\nThe dataset contains 6 columns\r\nRead in the labels worksheet for the column dictionary\r\n\r\n\r\ndta_labels <- readxl::read_xlsx(path, sheet = \"labels\")\r\n\r\ndta_labels\r\n\r\n\r\n# A tibble: 6 x 2\r\n  label                              name      \r\n  <chr>                              <chr>     \r\n1 id                                 id        \r\n2 Which restaurant have you visited? restaurant\r\n3 I am happy with the greeting       greeting  \r\n4 I enjoyed the food                 food      \r\n5 I am  satisfied with the cost      cost      \r\n6 How satisfied you are overall      overall   \r\n\r\nNow,\r\nChange the column names of the dataset to the short names in the dictionary\r\nSet the labels of the columns via the labelled::var_label() function\r\nCreate a label_list object, which is a named list where the names of the list are variable names and the values of the list are the variable labels\r\nAlso, save a label_vector named vector with the same informationas the label_list\r\n\r\n\r\nnames(dta) <- dta_labels$name\r\n\r\nvar_label(dta) <- dta_labels$label\r\n\r\nlabels_list <- var_label(dta)\r\nlabels_vector <-  var_label(dta, unlist = TRUE)\r\n\r\nlabels_list\r\n\r\n\r\n$id\r\n[1] \"id\"\r\n\r\n$restaurant\r\n[1] \"Which restaurant have you visited?\"\r\n\r\n$greeting\r\n[1] \"I am happy with the greeting\"\r\n\r\n$food\r\n[1] \"I enjoyed the food\"\r\n\r\n$cost\r\n[1] \"I am  satisfied with the cost\"\r\n\r\n$overall\r\n[1] \"How satisfied you are overall\"\r\n\r\nHave a look at the dataset names now\r\n\r\n\r\nglimpse(dta)\r\n\r\n\r\nRows: 7\r\nColumns: 6\r\n$ id         <dbl> 1, 2, 3, 4, 5, 6, 7\r\n$ restaurant <chr> \"Limassol\", \"Nicosia\", \"Nicosia\", \"Larnaka\", \"Mal~\r\n$ greeting   <chr> \"Agree\", \"Strongly disagree\", \"Neutral\", \"Neutral~\r\n$ food       <chr> \"Neutral\", \"Neutral\", \"Strongly agree\", \"Neutral\"~\r\n$ cost       <chr> \"Agree\", \"Strongly disagree\", \"Agree\", \"Strongly ~\r\n$ overall    <chr> \"Very satisfied\", \"Very dissatisfied\", \"Neutral\",~\r\n\r\nNow its very easy to keep using the variable names in your code, but variable labels are rendered in the tables when using {gtsummary} for example.\r\n\r\n\r\ndta %>%\r\n  select(overall,restaurant) %>% \r\n  tbl_summary(\r\n    by  = restaurant\r\n  ) \r\n\r\n\r\n\r\nCharacteristic\r\n      Larnaka, N = 11\r\n      Limassol, N = 21\r\n      Mall, N = 11\r\n      Nicosia, N = 21\r\n      Pafos, N = 11\r\n    How satisfied you are overall\r\n\r\n\r\n\r\n\r\nDissatisfied\r\n0 (0%)\r\n0 (0%)\r\n1 (100%)\r\n0 (0%)\r\n0 (0%)Neutral\r\n0 (0%)\r\n0 (0%)\r\n0 (0%)\r\n1 (50%)\r\n1 (100%)Satisfied\r\n1 (100%)\r\n0 (0%)\r\n0 (0%)\r\n0 (0%)\r\n0 (0%)Very dissatisfied\r\n0 (0%)\r\n0 (0%)\r\n0 (0%)\r\n1 (50%)\r\n0 (0%)Very satisfied\r\n0 (0%)\r\n2 (100%)\r\n0 (0%)\r\n0 (0%)\r\n0 (0%)\r\n        \r\n          1\r\n          \r\n           \r\n          n (%)\r\n          \r\n      \r\n    \r\n\r\nFactors and order of the value labels\r\nThe order of the satifaction levels in the previous table, is not the most appropriate. It should read from Very dissatisfied down to Very satisfied.\r\nOne way to accomplish this is to turn the variable into a factor. However, one problem arises now, where the label for the overall variable is gone. That is an inherent behaviour of dplyr::mutate() where it removes these attributes off of the variables that you manipulate.\r\nSee the overall title in the table below\r\n\r\n\r\n# change `overall` to factor\r\ndta <- \r\n  dta %>%\r\n  mutate(\r\n    overall  =  factor(overall,levels  = c(\"Very dissatisfied\", \"Dissatisfied\", \"Neutral\",\r\n                                           \"Satisfied\",\"Very satisfied\"))\r\n  ) \r\n\r\ndta %>% \r\n  select(overall,restaurant) %>% \r\n  tbl_summary(\r\n    by  = restaurant\r\n  ) \r\n\r\n\r\n\r\nCharacteristic\r\n      Larnaka, N = 11\r\n      Limassol, N = 21\r\n      Mall, N = 11\r\n      Nicosia, N = 21\r\n      Pafos, N = 11\r\n    overall\r\n\r\n\r\n\r\n\r\nVery dissatisfied\r\n0 (0%)\r\n0 (0%)\r\n0 (0%)\r\n1 (50%)\r\n0 (0%)Dissatisfied\r\n0 (0%)\r\n0 (0%)\r\n1 (100%)\r\n0 (0%)\r\n0 (0%)Neutral\r\n0 (0%)\r\n0 (0%)\r\n0 (0%)\r\n1 (50%)\r\n1 (100%)Satisfied\r\n1 (100%)\r\n0 (0%)\r\n0 (0%)\r\n0 (0%)\r\n0 (0%)Very satisfied\r\n0 (0%)\r\n2 (100%)\r\n0 (0%)\r\n0 (0%)\r\n0 (0%)\r\n        \r\n          1\r\n          \r\n           \r\n          n (%)\r\n          \r\n      \r\n    \r\n\r\nYou can easily solve this, using the labelled::set_variable_names() function. This function can be used in the dplyr chain since it takes a data argument.\r\nWhat you need to use is the labels_list we have saved before and pass it in the .labels argument of the set_variable_names()\r\n\r\n\r\ndta <- \r\n  dta %>%\r\n  set_variable_labels(.labels=labels_list)  #After mutating, the label information of the `overall` variable is gone.\r\n\r\ndta %>% \r\n  select(overall,restaurant) %>% \r\n  tbl_summary(\r\n    by  = restaurant\r\n  ) \r\n\r\n\r\n\r\nCharacteristic\r\n      Larnaka, N = 11\r\n      Limassol, N = 21\r\n      Mall, N = 11\r\n      Nicosia, N = 21\r\n      Pafos, N = 11\r\n    How satisfied you are overall\r\n\r\n\r\n\r\n\r\nVery dissatisfied\r\n0 (0%)\r\n0 (0%)\r\n0 (0%)\r\n1 (50%)\r\n0 (0%)Dissatisfied\r\n0 (0%)\r\n0 (0%)\r\n1 (100%)\r\n0 (0%)\r\n0 (0%)Neutral\r\n0 (0%)\r\n0 (0%)\r\n0 (0%)\r\n1 (50%)\r\n1 (100%)Satisfied\r\n1 (100%)\r\n0 (0%)\r\n0 (0%)\r\n0 (0%)\r\n0 (0%)Very satisfied\r\n0 (0%)\r\n2 (100%)\r\n0 (0%)\r\n0 (0%)\r\n0 (0%)\r\n        \r\n          1\r\n          \r\n           \r\n          n (%)\r\n          \r\n      \r\n    \r\n\r\nImportant to say that:\r\nUsing the labels_list object we do not worry about the order of the variables as the matchingis done on the names of the variables.\r\nThere is no problem if the data has more variables than what the labels_list contains as long as all the variables contained in the labels_list are also in the dataset we want to label\r\nPlots\r\nAxis titles\r\nHere, we want to utilise the labels we stored on the labels_list when defining axes title labels.\r\nHere is the plot with the default varible names as the xes title labels\r\n\r\n\r\nplot_overall <- \r\n  dta %>%\r\n  count(overall) %>% \r\n  ggplot(aes(overall, n))+\r\n  geom_col()+\r\n  coord_flip()\r\n\r\nplot_overall\r\n\r\n\r\n\r\n\r\nNow, you don;t need to type in manually the axis title label. Just use the labels_list\r\n\r\n\r\nplot_overall  +\r\n  labs(x=labels_list[[\"overall\"]])\r\n\r\n\r\n\r\n\r\nFacet strip labels\r\nHere, we will utilise the labels_vector to show descriptive labels in the facet_warp strip texts\r\nUsually, with these kind of questions(a series of likert scale questions), I want to plot many questions at the same plot. One way to do this is by tranforming all the data into a long format and get the counts (or proportions) of ech response to each question in a long table. Then we plot it by faceting the graph by the question.\r\n\r\n\r\nfaceted_plot <- \r\n  dta %>%\r\n  select(greeting, food, cost) %>% \r\n  pivot_longer(everything(), \r\n               names_to = \"attribute\",\r\n               values_to= \"rating\") %>% \r\n  count(attribute,  rating) %>% \r\n  mutate(\r\n    rating = factor(rating, c(\"Strongly agree\", \"Agree\", \"Neutral\", \"Disagree\", \"Strongly disagree\"))\r\n  ) %>% \r\n  ggplot(aes(rating, n))+\r\n  geom_col()+\r\n  coord_flip()+\r\n  facet_wrap(~attribute)\r\n\r\nfaceted_plot\r\n\r\n\r\n\r\n\r\nWhen using facet wrap in ggplots we get the values of the facet column in the strip text. However, since the facet column is actually a column that holds the question, it would be great to have the long label in the facet strip instead of the short name.\r\nThankfully we don’t have to do this manually. We can utilise the label_vector we saved earlier, and the underated ggplot function as_labeller (or sometimes the labeller function).You can read more about as_labeller and labeller for many more (and more usufull) use cases.\r\n\r\n\r\nfaceted_plot+\r\n  facet_wrap(~attribute,labeller = as_labeller(labels_vector))\r\n\r\n\r\n\r\n\r\nAxis text labels\r\nHere we will change how the levels in the axis ticks are displayed.\r\nIm my workflow, very often I need to plot proportions (or other summary statistic) of many items in the survey. Let’s assume we need to plot the proportion of respondents who answered positively ie.. Strongly agree or Agree in the rating questions greeting, food and cost.\r\nOne way of doing this:\r\n-1. Gather all rating questions (lets call them attributes) into a long format table with 2 columns - attribute and the rating for that attribute. -2. Indicate if the rating is positive ie. Agree/ Strongly agree -3. Count tne occurrences of positive rating\r\n\r\n\r\n# Table of good rating percentages\r\n\r\ntbl_pct <- \r\n  dta %>% \r\n  select(greeting, food, cost) %>% \r\n  # step 1\r\n  pivot_longer(everything(), \r\n               names_to = \"attribute\",\r\n               values_to= \"rating\") %>% \r\n  # 2. If Agree/strongly agree then `1` or `0`\r\n  mutate(rating  =  if_else(rating %in% c(\"Agree\",\"Strongly agree\"),1,  0)) %>% \r\n  # 3.Count the occurrences andget proportion\r\n  count(attribute, rating) %>% \r\n  group_by(attribute) %>% \r\n  mutate(pct = n/ sum(n)) \r\n\r\n#the plot\r\nplot_pct <- \r\n  tbl_pct %>% \r\n  # keep only the `1`s i.e. the proportion of agree/sstrongly agree\r\n  filter(rating ==1) %>% \r\n  ggplot(aes(attribute, pct))+\r\n  geom_col()\r\n\r\nplot_pct\r\n\r\n\r\n\r\n\r\nNow, we need to change the cost, food and greeting with appropriate text\r\nI usually use the scale_*_discrete function. There you can define the labels as a named list or a named vector\r\n\r\n\r\nplot_pct+\r\n  scale_x_discrete(labels = labels_list) # or labels_vector. Both will work\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-12-23-a-workflow-with-labelled-data/a-workflow-with-labelled-data_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2021-10-17T12:15:17+03:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-05-10-standardised-mortality-ratio/",
    "title": "Standardised Mortality Ratio",
    "description": "Calculation  of the SMR and and confidence interval",
    "author": [
      {
        "name": "Lefkios  Paikousis",
        "url": "https://www.linkedin.com/in/lefkios/"
      }
    ],
    "date": "2019-05-10",
    "categories": [
      "purrr",
      "regex",
      "excel",
      "rowwise"
    ],
    "contents": "\r\n\r\nContents\r\nWhat you can learn\r\nThe SMR\r\nRead file\r\nAggregate\r\nThe Confidence Interval\r\n\r\nThis is a reminder to myself (and to whoever finds this useful) on\r\nthe calculation of:\r\nthe Standardised\r\nMortality Ratio (SMR)\r\nThe Confidence Interval of the SMR\r\nWhat you can learn\r\nWhat you can learn in this reminder\r\nSome use of regular expressions\r\ncreating functions and use them to avoid repeat myself\r\nRead (multiple) excel data into R\r\nFunction programming basics using the package purrr, intro,\r\nusefull\r\nThe rowwise function\r\nlist.files function and the set_names from\r\npurrr to make it easy to map the files into functions and\r\nkeep a record of what comes from\r\nThe SMR\r\nThe SMR is calculated as the ratio of observed hospital mortality\r\nover predicted hospital mortality.\r\nI have data on ~1000 patients of the iCU and the following info:\r\nSAPS II score\r\nICU mortality (death/alive)\r\nThe data are in separate .rds file my working\r\ndirectory\r\nRead file\r\n\r\n\r\nlibrary(tidyverse)\r\n\r\nlocation <- \"_posts/2019-05-10-standardised-mortality-ratio/\"\r\n\r\ndta <- readRDS(here::here(location, \"dta.rds\"))\r\n\r\n\r\n\r\nHere is a bit of an output\r\n\r\n\r\ndta %>% \r\n  head(10) %>% \r\n  knitr::kable()\r\n\r\n\r\nyear\r\ndeath\r\nsaps_ii\r\ntype_of_admision\r\n2012\r\n0\r\n49\r\nmedical\r\n2012\r\n0\r\n24\r\nUnschedule surgical\r\n2012\r\n0\r\n84\r\nmedical\r\n2012\r\n0\r\n50\r\nmedical\r\n2012\r\n0\r\n12\r\nmedical\r\n2012\r\n1\r\n64\r\nmedical\r\n2012\r\n0\r\n31\r\nUnschedule surgical\r\n2012\r\n0\r\n35\r\nmedical\r\n2012\r\n0\r\n63\r\nmedical\r\n2012\r\n0\r\n26\r\nmedical\r\n\r\nAggregate\r\nLets aggregate by year, the observed mortality, and the median SAPS\r\nII score\r\n\r\n\r\n  dta_aggr = \r\n  dta %>% \r\n    group_by(year) %>% \r\n    summarise(n = n(),\r\n              obs_mortality = sum(death),\r\n              mortality_rate = mean(death),\r\n              # I get the median SAPS score\r\n              med_saps = median(saps_ii)\r\n              ) \r\n\r\n  dta_aggr %>% knitr::kable()\r\n\r\n\r\nyear\r\nn\r\nobs_mortality\r\nmortality_rate\r\nmed_saps\r\n2012\r\n144\r\n26\r\n0.1805556\r\n48\r\n2013\r\n165\r\n33\r\n0.2000000\r\n47\r\n2014\r\n159\r\n28\r\n0.1761006\r\n48\r\n2015\r\n198\r\n41\r\n0.2070707\r\n49\r\n2016\r\n184\r\n32\r\n0.1739130\r\n60\r\n2017\r\n216\r\n47\r\n0.2175926\r\n57\r\n\r\nNow I need to calculate the predicted mortality by year.\r\nThe predicted mortality is derived from the\r\nSAPS II score as follows:\r\n\\(logit = -7.7631 + 0.0737*Score +\r\n0.9971*ln(Score+1)\\)\r\nand then\r\n\\(Mortality =\r\n\\frac{e^{logit}}{1+e^{logit}}\\)\r\nHere I need a function that has the saps II score as an input and\r\nreturns the predicted mortality\r\n\r\n\r\npredict_mortality = function(score){\r\n  logit = -7.7631 + 0.0737 * score + 0.9971 * log(score+1)\r\n  mortality = exp(logit)/ (1+exp(logit))\r\n  return(mortality)\r\n}\r\n\r\n\r\n\r\nNow lets use our function predict_mortality to calculate\r\nthe predicted mortality out of the SAPS II score\r\n\r\n\r\n  smr_table = \r\n  dta_aggr %>% \r\n    mutate(pred_mortality_rate = predict_mortality(med_saps),\r\n           # get expecte counts of deaths\r\n           pred_mortality = pred_mortality_rate * n,\r\n           # The SMR \r\n           smr = obs_mortality/ pred_mortality) \r\n\r\n  #Have  a look\r\n  smr_table %>% knitr::kable(digits = 2)\r\n\r\n\r\nyear\r\nn\r\nobs_mortality\r\nmortality_rate\r\nmed_saps\r\npred_mortality_rate\r\npred_mortality\r\nsmr\r\n2012\r\n144\r\n26\r\n0.18\r\n48\r\n0.41\r\n59.70\r\n0.44\r\n2013\r\n165\r\n33\r\n0.20\r\n47\r\n0.39\r\n64.67\r\n0.51\r\n2014\r\n159\r\n28\r\n0.18\r\n48\r\n0.41\r\n65.92\r\n0.42\r\n2015\r\n198\r\n41\r\n0.21\r\n49\r\n0.44\r\n86.63\r\n0.47\r\n2016\r\n184\r\n32\r\n0.17\r\n60\r\n0.68\r\n125.28\r\n0.26\r\n2017\r\n216\r\n47\r\n0.22\r\n57\r\n0.62\r\n133.76\r\n0.35\r\n\r\nThe Confidence Interval\r\nAn approximate 95% confidence interval (CI) for the SMR was\r\ncalculated by using the method proposed by Vandenbroucke JP. A\r\nshortcut method for calculating the 95 percent confidence interval of\r\nthe standardized mortality ratio. (Letter). Am J Epidemiol 1982;\r\n115:303-4.\r\nI found the formulas of all possible CI calculations for the SMR here. It is the\r\ndocumentation of the this\r\nand this online\r\ncalculators\r\nI tried out a few formulas in the documentation, specifically the\r\nones that are called approximations\r\nI haven’t tried the Exact Tests calculations - I think they need more\r\nprogramming + I couldn’t figure out how I am supposed to do the\r\niterative process. Perhaps its easy and don’t have time to think about\r\nit :)\r\nWell i tried this approximation by Vanderbroucke\r\n\r\nBeware that the \\(\\sqrt(α)\\) refers\r\nto the observed mortality and the \\(λ\\)\r\nto the predicted mortality. DO NOT confuse with the \\(α\\) of the Z score (i.e. the 1.96)\r\nSo I created this function, that reads the observed and predicted\r\nmortality (actual counts) and returns a vector of 2 values. The 1st is\r\nthe lower limit, and the 2nd the upper limit\r\n\r\n\r\nsmr_conf =  function(observed, predicted){\r\n  \r\n  lower = ((sqrt(observed) - 1.96*0.5)^2)/ predicted\r\n  upper = ((sqrt(observed) + 1.96*0.5)^2)/ predicted\r\n  \r\n  return(c(lower, upper))\r\n  \r\n}\r\n\r\n\r\n\r\nNow lets use the smr_table we calculated earlier\r\nNOTE HERE the use of rowwise function\r\nof dplyr\r\nThe rowwise is needed since the inputs to the the\r\nfunction are taken rowwise\r\n\r\n\r\nsmr_table %>% \r\n  rowwise() %>% \r\n  mutate( lower_95 = smr_conf(obs_mortality, pred_mortality)[1],\r\n          upper_95 = smr_conf(obs_mortality, pred_mortality)[2]) %>% \r\n  knitr::kable(digits = 2)\r\n\r\n\r\nyear\r\nn\r\nobs_mortality\r\nmortality_rate\r\nmed_saps\r\npred_mortality_rate\r\npred_mortality\r\nsmr\r\nlower_95\r\nupper_95\r\n2012\r\n144\r\n26\r\n0.18\r\n48\r\n0.41\r\n59.70\r\n0.44\r\n0.28\r\n0.62\r\n2013\r\n165\r\n33\r\n0.20\r\n47\r\n0.39\r\n64.67\r\n0.51\r\n0.35\r\n0.70\r\n2014\r\n159\r\n28\r\n0.18\r\n48\r\n0.41\r\n65.92\r\n0.42\r\n0.28\r\n0.60\r\n2015\r\n198\r\n41\r\n0.21\r\n49\r\n0.44\r\n86.63\r\n0.47\r\n0.34\r\n0.63\r\n2016\r\n184\r\n32\r\n0.17\r\n60\r\n0.68\r\n125.28\r\n0.26\r\n0.17\r\n0.35\r\n2017\r\n216\r\n47\r\n0.22\r\n57\r\n0.62\r\n133.76\r\n0.35\r\n0.26\r\n0.46\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-07-31T11:39:32+03:00",
    "input_file": {}
  }
]
